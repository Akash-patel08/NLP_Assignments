{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"I study machine learning\",\n",
        "    \"Machine learning is fun\",\n",
        "    \"I study deep learning\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bag of Words - Count Occurence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsD8HaZjpiG",
        "outputId": "a27c8a9a-40c0-4e73-ec7b-fa318a1f4cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['deep' 'fun' 'is' 'learning' 'machine' 'study']\n",
            "Count Matrix:\n",
            " [[0 0 0 1 1 1]\n",
            " [0 1 1 1 1 0]\n",
            " [1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_counts = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Count Matrix:\\n\", bow_counts.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bag of Words – Normalized Count Occurrence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PFLQ19ojwqa",
        "outputId": "faefc23d-7d98-4975-d710-756b77c3e657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized BoW:\n",
            " [[0.         0.         0.         0.33333333 0.33333333 0.33333333]\n",
            " [0.         0.25       0.25       0.25       0.25       0.        ]\n",
            " [0.33333333 0.         0.         0.33333333 0.         0.33333333]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "count_matrix = bow_counts.toarray()\n",
        "normalized_bow = count_matrix / count_matrix.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"Normalized BoW:\\n\", normalized_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF (Term Frequency – Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0oWveS-kHoE",
        "outputId": "f441f1b0-65b7-4afd-ea2a-c6e30fd7af63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['deep' 'fun' 'is' 'learning' 'machine' 'study']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.48133417 0.61980538 0.61980538]\n",
            " [0.         0.5844829  0.5844829  0.34520502 0.44451431 0.        ]\n",
            " [0.72033345 0.         0.         0.42544054 0.         0.54783215]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fz5lohWtkNgL"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un04SoV3k4Sf",
        "outputId": "32f354f6-082f-4eb5-ba1e-5700aeb7b162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in c:\\users\\akash\\anaconda3\\lib\\site-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\akash\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CwQODJIilC0G"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_JmxS5BlJuE",
        "outputId": "e5e5f992-0090-4c1f-df98-245440ea7b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector size: 100\n",
            "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
          ]
        }
      ],
      "source": [
        "vector = w2v_model.wv[\"learning\"]\n",
        "print(\"Vector size:\", len(vector))\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRhgUj-NlJo6",
        "outputId": "ac786279-67ca-4231-e3a0-ba2c07bfab8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document embedding shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def document_vector(doc):\n",
        "    words = doc.lower().split()\n",
        "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "doc_embedding = document_vector(\"I study machine learning\")\n",
        "print(\"Document embedding shape:\", doc_embedding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "Bag-of-Words and TF-IDF represent text numerically, while Word2Vec captures semantic meaning using embeddings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
